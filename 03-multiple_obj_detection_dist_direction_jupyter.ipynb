{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a94ead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: opencv-contrib-python==4.5.3.56 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (4.5.3.56)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\kiit\\anaconda3\\lib\\site-packages (from opencv-contrib-python==4.5.3.56) (1.21.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-contrib-python==4.5.3.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "625c2911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv # computer vision library for image processing\n",
    "import pyttsx3 # for text to speech conversion\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe8d0689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Width = 640\n",
      "Width/3 = 213\n"
     ]
    }
   ],
   "source": [
    "# Opening the camera to check its pixel width\n",
    "# Dividing the camera frame into 3 vertical parts/strips (left/middle/right). \n",
    "# Used later for sensing object direction wrt the camera screen\n",
    "\n",
    "n_rows = 1\n",
    "n_images_per_row = 3\n",
    "\n",
    "cap = cv.VideoCapture(0)\n",
    "\n",
    "ret, frame = cap.read()\n",
    "height,width,ch = frame.shape\n",
    "\n",
    "roi_height = height // n_rows\n",
    "roi_width = width // n_images_per_row\n",
    "\n",
    "print(\"Width =\", width)\n",
    "print(\"Width/3 =\", roi_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c594aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_speech(string):\n",
    "    text_speech = pyttsx3.init()\n",
    "    text_speech.say(string)\n",
    "    text_speech.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba3d91d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# object detector function /method\n",
    "def object_detector(image):\n",
    "    classes, scores, boxes = model.detect(image, CONFIDENCE_THRESHOLD, NMS_THRESHOLD)\n",
    "    # creating empty list to add objects data\n",
    "    data_list = []\n",
    "    for (classid, score, box) in zip(classes, scores, boxes):\n",
    "        \n",
    "        # define color of each, object based on its class id\n",
    "        '''\n",
    "        80 object names are stored in classes.txt and 6 colours are stored in var COLORS\n",
    "        For 1st object name (person) -> 0 % 6 = 0. So colour[0] = RED will be chosen\n",
    "        For 2nd object name (bicycle) -> 1 % 6 = 1. So colour[1] = PURPLE will be chosen\n",
    "        And so on...\n",
    "        '''\n",
    "        color= COLORS[int(classid) % len(COLORS)]\n",
    "        \n",
    "        # label -> stores class id and confidence score\n",
    "        label = \"%s : %f\" % (class_names[classid[0]], score)\n",
    "        print(label)\n",
    "\n",
    "        # draw rectangle on and label on object\n",
    "        cv.rectangle(image, box, color, 2)\n",
    "        cv.putText(image, label, (box[0], box[1]-14), FONTS, 0.5, color, 2)\n",
    "        \n",
    "        # print(\"box values =\", box)\n",
    "        # prints [  3  67 425 413] (Example)\n",
    "        '''\n",
    "        3 is the x-coordinate of the top-left corner of the bounding box.\n",
    "        67 is the y-coordinate of the top-left corner of the bounding box.\n",
    "        425 is the x-coordinate of the bottom-right corner of the bounding box.\n",
    "        413 is the y-coordinate of the bottom-right corner of the bounding box.\n",
    "        '''\n",
    "        \n",
    "        # finding the central coordinates of bounding box\n",
    "        mid_x = (box[0] + box[2]) / 2\n",
    "        # mid_y = (box[1] + box[3]) / 2\n",
    "        \n",
    "        # plotting a point at the centre of the bounding box\n",
    "        # cv.circle(frame, (mid_x, mid_y), 5, (255, 0, 255), cv.FILLED)\n",
    "        \n",
    "        # finding the x coordinates of the frames \n",
    "        right_end_of_frame1 = roi_width\n",
    "        right_end_of_frame2 = 2*roi_width\n",
    "        # right_end_of_frame3 = right_end_of_frame2 + roi_width\n",
    "        \n",
    "        #print(\"Right end of frame 1:\", right_end_of_frame1)\n",
    "        #print(\"Right end of frame 2:\", right_end_of_frame2)\n",
    "        \n",
    "        # if the central coordinates lie in the the left/right/middle frame\n",
    "        if(mid_x < right_end_of_frame1):\n",
    "            text_to_speech('SLIGHTLY ON YOUR LEFT')\n",
    "        elif(mid_x > right_end_of_frame1 and mid_x < right_end_of_frame2):\n",
    "            text_to_speech('IN FRONT OF YOU')\n",
    "        elif(mid_x > right_end_of_frame2):\n",
    "            text_to_speech('SLIGHTLY ON YOUR RIGHT')\n",
    "        \n",
    "        # getting the data \n",
    "        # 1: class name  2: object width in pixels, 3: position where have to draw text(distance)\n",
    "        if classid == 0: # person class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 67: # cell phone class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        \n",
    "        elif classid == 1: # bicycle class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 2: # car class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 3: # motorbike class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 5: # bus class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 6: # train class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 7: # truck class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 13: # bench class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 15: # cat class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 16: # dog class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 19: # cow class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 39: # bottle class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 56: # chair class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 57: # sofa class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 59: # bed class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 60: # dining table class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 61: # toilet class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 62: # tvmonitor class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        elif classid == 63: # laptop class id\n",
    "            data_list.append([class_names[classid[0]], box[2], (box[0], box[1]-2)])\n",
    "        \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "853b5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR FINDING FOCAL LENGTH OF CAM LENS\n",
    "def focal_length_finder (measured_distance, real_width, width_in_rf):\n",
    "    focal_length = (width_in_rf * measured_distance) / real_width\n",
    "    print(\"FOCAL LENGTH: \", focal_length)\n",
    "    # focal_length = 560\n",
    "    return focal_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ff8401",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR FINDING DISTANCE OF OBJECT FROM CAM\n",
    "def distance_finder(focal_length, real_object_width, width_in_frame):\n",
    "    distance = (real_object_width * focal_length) / width_in_frame\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dc4e671",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance constants \n",
    "KNOWN_DISTANCE = 114.3 #CENTIMETERS\n",
    "PERSON_WIDTH = 37 #CENTIMETERS\n",
    "MOBILE_WIDTH = 7 #CENTIMETERS\n",
    "BOTTLE_WIDTH = 8\n",
    "BICYCLE_WIDTH = 67.5\n",
    "CAR_WIDTH = 177\n",
    "MOTORBIKE_WIDTH = 82.5\n",
    "BUS_WIDTH = 90\n",
    "BENCH_WIDTH = 129.5\n",
    "CAT_WIDTH = 46\n",
    "DOG_WIDTH = 51\n",
    "COW_WIDTH = 183\n",
    "CHAIR_WIDTH = 43\n",
    "SOFA_WIDTH = 150\n",
    "BED_WIDTH = 190\n",
    "DINING_TABLE_WIDTH = 96.5\n",
    "TOILET_WIDTH = 51\n",
    "TV_WIDTH = 100\n",
    "LAPTOP_WIDTH = 14\n",
    "TRUCK_WIDTH = 198\n",
    "\n",
    "# Object detector constant \n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "NMS_THRESHOLD = 0.3\n",
    "\n",
    "# colors for object detected\n",
    "COLORS = [(255,0,0),(255,0,255),(0, 255, 255), (255, 255, 0), (0, 255, 0), (255, 0, 0)]\n",
    "# COLORS = [red, purple, cyan, brown, green, red] \n",
    "\n",
    "GREEN =(0,255,0) # To show the distance text above the object in GREEN\n",
    "BLACK =(0,0,0) # To show the bounding box (rectangle) in BLACK\n",
    "\n",
    "# defining fonts \n",
    "FONTS = cv.FONT_HERSHEY_COMPLEX\n",
    "\n",
    "# getting class names from classes.txt file \n",
    "class_names = []\n",
    "#with open(\"C:\\\\Users\\\\KIIT\\\\MyPython\\\\PROJECTS\\\\object_detection+dist\\\\Yolov4-Detector-and-Distance-Estimator-master\\\\classes.txt\", \"r\") as f:\n",
    "with open(\"classes.txt\", \"r\") as f:\n",
    "    class_names = [cname.strip() for cname in f.readlines()]\n",
    "\n",
    "# setting up opencv net\n",
    "\n",
    "yoloNet = cv.dnn.readNet('yolov4-tiny.weights', 'yolov4-tiny.cfg')\n",
    "# Reads deep learning network represented in one of the supported formats.\n",
    "# For more info, check the links below:\n",
    "# https://docs.opencv.org/3.4/d6/d0f/group__dnn.html#ga3b34fe7a29494a6a4295c169a7d32422\n",
    "# https://stackoverflow.com/questions/50390836/understanding-darknets-yolo-cfg-config-files\n",
    "\n",
    "'''\n",
    "yoloNet.setPreferableBackend(cv.dnn.DNN_BACKEND_CUDA): This line sets the preferable backend for the YOLO \n",
    "neural network to CUDA. CUDA is a parallel computing platform and application programming interface (API) \n",
    "model created by NVIDIA. Using CUDA can significantly accelerate computations on NVIDIA GPUs (Graphics \n",
    "Processing Units). In this context, it means that the YOLO network will try to utilize CUDA for faster inference.\n",
    "\n",
    "yoloNet.setPreferableTarget(cv.dnn.DNN_TARGET_CUDA_FP16): This line sets the preferable target for the YOLO \n",
    "neural network to CUDA with FP16 precision. FP16 (half-precision floating-point) is a data type that uses 16 \n",
    "bits to represent a floating-point number, providing a compromise between precision and computational efficiency. \n",
    "Using FP16 can further speed up computations on compatible GPUs.\n",
    "'''\n",
    "yoloNet.setPreferableBackend(cv.dnn.DNN_BACKEND_CUDA)\n",
    "yoloNet.setPreferableTarget(cv.dnn.DNN_TARGET_CUDA_FP16)\n",
    "\n",
    "model = cv.dnn_DetectionModel(yoloNet)\n",
    "\n",
    "'''\n",
    "scale -> scale factor (1/255 to scale the pixel values to [0..1])\n",
    "size -> 416x416 square image\n",
    "\n",
    "(swapBR=True) -> since OpenCV uses BGR. \n",
    "    OpenCV assumes images are in BGR channel order; \n",
    "    however, the `mean` value assumes we are using RGB order. \n",
    "    To resolve this discrepancy we can swap the R and B channels in image\n",
    "    by setting this value to `True`.\n",
    "    By default OpenCV performs this channel swapping for us.\n",
    "'''\n",
    "model.setInputParams(size=(416, 416), scale=1/255, swapRB=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5d49acc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person : 0.887720\n",
      "[['person', 440, (50, 134)]]\n",
      "cell phone : 0.923403\n",
      "[['person', 440, (50, 134)], ['cell phone', 72, (206, 296)]]\n",
      "[['person', 440, (50, 134)], ['cell phone', 72, (206, 296)]]\n",
      "person : 0.910768\n",
      "[['person', 367, (135, 134)]]\n",
      "[['person', 367, (135, 134)]]\n",
      "Person width in pixels : 367) mobile width in pixel: 72\n",
      "FOCAL LENGTH:  1133.7324324324325\n"
     ]
    }
   ],
   "source": [
    "#FINDING THE FOCAL LENGTH THROUGH THE REFERENCE IMAGES\n",
    "# reading the reference image from dir \n",
    "ref_person = cv.imread('ReferenceImages/image14.png')\n",
    "ref_mobile = cv.imread('ReferenceImages/image4.png')\n",
    "\n",
    "mobile_data = object_detector(ref_mobile)\n",
    "'''\n",
    "displays [['person', 440, (50, 134)], ['cell phone', 72, (206, 296)]]\n",
    "440 -> pixel width of person ; (50, 134) -> coordinates of person in image\n",
    "72 -> pixel width of person ; (206, 296) -> coordinates of cell phone in image\n",
    "'''\n",
    "print(mobile_data)\n",
    "'''\n",
    "[['person', 440, (50, 134)], ['cell phone', 72, (206, 296)]]\n",
    "If we check indices [1][1] => 72\n",
    "So mobile_data[1][1] = 72\n",
    "'''\n",
    "mobile_width_in_rf = mobile_data[1][1]\n",
    "person_data = object_detector(ref_person)\n",
    "\n",
    "'''\n",
    "displays [['person', 367, (135, 134)]]\n",
    "'''\n",
    "print(person_data)\n",
    "person_width_in_rf = person_data[0][1]\n",
    "\n",
    "print(f\"Person width in pixels : {person_width_in_rf}) mobile width in pixel: {mobile_width_in_rf}\")\n",
    "# finding focal length \n",
    "focal_length = focal_length_finder(KNOWN_DISTANCE, PERSON_WIDTH, person_width_in_rf)\n",
    "#focal_length_mobile = focal_length_finder(KNOWN_DISTANCE, MOBILE_WIDTH, mobile_width_in_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b36ca28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_announce_store_dist(obj_info, width):\n",
    "    # print(obj_info)\n",
    "    # typecasting to int and removing decimals\n",
    "    dist_cm = distance_finder(focal_length, width, obj_info[1])\n",
    "    dist_m = dist_cm / 100\n",
    "    # use to limit no. of digits after decimal\n",
    "    approx_dist = round(dist_m, 2)\n",
    "    dist_text = f'{obj_info[0]} detected {approx_dist} meters'\n",
    "    text_to_speech(dist_text)\n",
    "    return approx_dist # required for drawing bounding box later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6047542f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef split_rename_screen(n_rows, n_images_per_row, roi_height, roi_width):\\n    \\n    images = []\\n        \\n    for x in range (0,n_rows):\\n        for y in range (0,n_images_per_row):\\n            \\n#             [[[213 202 199]\\n#               [212 201 199]\\n#               [212 202 201]\\n#               ...\\n#               [226 210 211]\\n#               [226 210 210]\\n#               [224 209 209]]\\n    \\n            tmp_image = frame[int (x*roi_height) : int ((x+1)*roi_height), int (y*roi_width) : int ((y+1)*roi_width)]\\n            images.append(tmp_image)\\n            #print(tmp_image)\\n    \\n    # Display the resulting sub-frame\\n    direction_names = ['left','center','right'] # reference for determining the direction \\n    direction = 0 # pointer to iterate the direction array\\n    for x in range(0, n_rows):\\n        for y in range(0, n_images_per_row):\\n                \\n            cv.imshow(direction_names[direction] , images[x*n_images_per_row+y])\\n            cv.moveWindow(direction_names[direction], 100+(y*roi_width), 50+(x*roi_height))\\n            direction = direction + 1\\n                \\n            # cv.imshow(str (x*n_images_per_row+y+1), images[x*n_images_per_row+y])\\n            # cv.moveWindow(str (x*n_images_per_row+y+1), 100+(y*roi_width), 50+(x*roi_height))\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def split_rename_screen(n_rows, n_images_per_row, roi_height, roi_width):\n",
    "    \n",
    "    images = []\n",
    "        \n",
    "    for x in range (0,n_rows):\n",
    "        for y in range (0,n_images_per_row):\n",
    "            \n",
    "#             [[[213 202 199]\n",
    "#               [212 201 199]\n",
    "#               [212 202 201]\n",
    "#               ...\n",
    "#               [226 210 211]\n",
    "#               [226 210 210]\n",
    "#               [224 209 209]]\n",
    "    \n",
    "            tmp_image = frame[int (x*roi_height) : int ((x+1)*roi_height), int (y*roi_width) : int ((y+1)*roi_width)]\n",
    "            images.append(tmp_image)\n",
    "            #print(tmp_image)\n",
    "    \n",
    "    # Display the resulting sub-frame\n",
    "    direction_names = ['left','center','right'] # reference for determining the direction \n",
    "    direction = 0 # pointer to iterate the direction array\n",
    "    for x in range(0, n_rows):\n",
    "        for y in range(0, n_images_per_row):\n",
    "                \n",
    "            cv.imshow(direction_names[direction] , images[x*n_images_per_row+y])\n",
    "            cv.moveWindow(direction_names[direction], 100+(y*roi_width), 50+(x*roi_height))\n",
    "            direction = direction + 1\n",
    "                \n",
    "            # cv.imshow(str (x*n_images_per_row+y+1), images[x*n_images_per_row+y])\n",
    "            # cv.moveWindow(str (x*n_images_per_row+y+1), 100+(y*roi_width), 50+(x*roi_height))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fbd56bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person : 0.514328\n",
      "[['person', 426, (107, 118)]]\n",
      "person : 0.523907\n",
      "[['person', 449, (99, 63)]]\n",
      "person : 0.540398\n",
      "[['person', 491, (75, 124)]]\n",
      "person : 0.585566\n",
      "[['person', 505, (68, 121)]]\n",
      "person : 0.558009\n",
      "[['person', 381, (32, 70)]]\n",
      "person : 0.511805\n",
      "[['person', 478, (31, 68)]]\n",
      "person : 0.504179\n",
      "[['person', 482, (29, 68)]]\n",
      "person : 0.636484\n",
      "[['person', 483, (76, 125)]]\n",
      "person : 0.540434\n",
      "[['person', 498, (22, 121)]]\n",
      "person : 0.546428\n",
      "[['person', 506, (17, 122)]]\n"
     ]
    }
   ],
   "source": [
    "# ACTIVATING AND IMPLEMENTING IN CAMERA\n",
    "\n",
    "cap = cv.VideoCapture(0) # capturing using cam of pc\n",
    "n_rows = 1\n",
    "n_images_per_row = 3\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read() # inbuilt function for capturing image frame using camera\n",
    "    height,width,ch = frame.shape\n",
    "    \n",
    "    roi_height = height // n_rows\n",
    "    roi_width = width // n_images_per_row\n",
    "    \n",
    "    '''\n",
    "    if a person comes in front of the camera, variable data stores \n",
    "    [['person', 532, (54, 124)]]\n",
    "    Same for other objects\n",
    "    '''\n",
    "    data = object_detector(frame) \n",
    "    #print(data)\n",
    "    \n",
    "    for d in data:\n",
    "        \n",
    "        if d[0] == 'person':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, PERSON_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'cell phone':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, MOBILE_WIDTH)\n",
    "            \n",
    "        elif d[0] == 'bottle':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, BOTTLE_WIDTH)\n",
    "            \n",
    "        elif d[0] == 'bicycle':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, BICYCLE_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'car':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, CAR_WIDTH)\n",
    "            \n",
    "        elif d[0] == 'motorbike':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, MOTORBIKE_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'bus':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, BUS_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'train':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, TRAIN_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'truck':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, TRUCK_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'bench':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, BENCH_WIDTH)\n",
    "            \n",
    "        elif d[0] == 'cat':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, CAT_WIDTH)\n",
    "            \n",
    "        elif d[0] == 'dog':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, DOG_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'cow':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, COW_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'chair':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, CHAIR_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'sofa':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, SOFA_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'bed':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, BED_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'diningtable':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, DINING_TABLE_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'toilet':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, TOILET_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'tvmonitor':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, TV_WIDTH)\n",
    "        \n",
    "        elif d[0] == 'laptop':\n",
    "            x, y = d[2]\n",
    "            distance = calc_announce_store_dist(d, LAPTOP_WIDTH)\n",
    "        \n",
    "        # splits the screen into 3 vertical divisions\n",
    "        # split_rename_screen(n_rows, n_images_per_row, roi_height, roi_width)\n",
    "        \n",
    "        #announce_direction()\n",
    "        \n",
    "        #printf(\"Bounding Box: Left=%d, Top=%d, Right=%d, Bottom=%d\\n\", left, top, right, bot); \n",
    "        #draw_box_width(im, left, top, right, bot, width, red, green, blue);\n",
    "        \n",
    "        # black rectangle at the upper left corner of the bounding box to display object distance\n",
    "        cv.rectangle(frame, (x, y-3), (x+150, y+23), BLACK, -1)\n",
    "        # green text inside the rectangle\n",
    "        cv.putText(frame, f'DIST: {round(distance, 0)} M', (x+5,y+13), FONTS, 0.48, GREEN, 2)    \n",
    "        \n",
    "    cv.imshow('frame', frame)\n",
    "    \n",
    "    key = cv.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "cv.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d92045b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
